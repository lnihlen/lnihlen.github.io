---
layout: post
title: "Beach Day"
date: 2019-04-28
tags: [ personal, sclork ]
---

With the weather cleared up this morning the obvious best thing to do was to roll the family down to the beach. While
there were a lot of cars parked at the end of the residential street we usually park on to walk the short distance to
the beach, there were much fewer people out on the sand than one would expect for such a lovely day.

The tide was out so there was plenty of beach to roam between the waves and the cliffs that tend to rim the beaches all
along the coast in this area. Lots of crushed shells and tiny crab skeletons too. Our pit mix Storm loves to get in
the water and so I was the one who walked with her enough to get our feet wet. As is typical of the Pacific, the water
was cold enough to actually be a bit painful. Somehow I still want always to stand in the ocean, maybe it's owing to my
land-locked desert origins back in New Mexico.

Then home, and procrastination instead of diving in for additional work on the Confab project for
{% include tag_link.html tag="sclork" %}. It is, after all, Sunday, and all these videogames aren't just going to play
themselves. But I did eventually dig in, trying to devise some sort of caching or even overall memory organization
strategy for the database part of the program.

I think for a first iteration I'm going to just go with a completely naive caching strategy. One of the most frequent
use cases I can think of is asset sharing write after creation, which would mean that a hub-and-spoke sort of network
topology would see a lot of requests coming in at the same time for some assets. I note that LevelDB offers some simple
[LRU caching strategy](https://github.com/google/leveldb/blob/master/doc/index.md#cache) that one can put in front of
a database. I love how the example documentation casually allocates a 100MB of RAM for the cache. Probably not going
to be anywhere near that big. I'll get some actual data into the system and then see what kind of speedup I can get with
a few MB of cache. Since most of the assets I'd want to share quickly are likely code snippets, on the order of a few
kilobytes at most, I have a feeling even a cache of a few MB would be more than sufficient.

I didn't get to implementation, or even documentation of the decisions around caching, because I was also thinking a lot
about how to eliminate copies, mostly in asset retrieval. I notice that LevelDB exposes this Slice object, which is
essentially a non-owning pointer and a size, referring to a block of data. They cast to ```std::string``` and then a
copy is (almost) sure to happen, but if you keep a certain contract by not modifying the iterator that gave you the
slice on a read you can safely access the memory contained within without having to copy it to a buffer outside of the
library.

The next trick was trying to figure out how to convince yaml-cpp to parse the YAML input directly from the Slice. I
noticed the Load function was overloaded to take at ```std::istream&```, and after some Googling around and reading on
Stack Overflow (natch), I wound up looking at boost's Iostream library and the ```array_source``` contained within.
I was able to write some prototype code which allowed me to deserialize a YAML string back into a useful data structure
without any copies. I had a little time to start generalizing it but find it pretty promising.

I haven't messed with boost much and, quite frankly, was impressed by all the cool stuff in there. I'm halfway tempted,
before going too deep into code conventions with Google logging and flags, to swap over to some of the equivalent
libraries in boost. Particuarly because another hour or more of my precious dev time this evening was spent faffing
about with CMake and glog, trying to get it to either find gflags or give up and not find gflags but not make such a
giant fuss about it in the process.

There's this terse and unhelpful message in a [bug report from 2017](https://github.com/google/glog/issues/251) on glog
with the exact same issue I have, and the response from the developer is that ```ExternalProject``` is the right way to
add dependencies now in CMake, with none
of this ```add_subdirectory``` business us CMake neophytes are doing. Well, I took a read through the ExternalProject
documentation, even reading a blog posting by one of the CMake developers about it, and while ExternalProject does a
lot of impressive stuff my best understanding of it is that it's all about just getting the dependency in question
on the build computer. It's possible to have a CMake system so fancy that you can update your git submodules every
time you compile the project, or some such, thus following
[Abseil Live at Head](https://abseil.io/about/philosophy#we-recommend-that-you-choose-to-live-at-head) philosophy,
but I don't know. I guess I've now reached the age where having a git repository with submodules is considered *quaint*,
even old-fashioned. Why control the versions of your dependencies when CMake can do it for you?

Maybe my build system will check out all of my dependent code some day. It's true that the true purpose of submodules
is to allow you to edit all of the repositories and commit changes to all of them in a sane manner, meaning that these
kind of read-only modules are a bit of an abuse of the system, I'm more just using git submodules as a way of
controlling the version of the dependencies. It looks like ExternalProject could do that, and is even willing to
work with the submodules as I've set them up.

The thing is is that even that expert-level CMake article where the build system is now cooking you breakfast and
giving you a foot massage I notice that at the end of all of the ExternalProject magic there's still a good
old-fashioned ```add_subdirectory``` command asking CMake to dip in and parse the same exact CMakeLists.txt file that,
in glog's case, is the thing that's causing all of my problems. So, someone is wrong on the Internet! And it's probably
me!

In the end I have glog building without even trying to find gflags, and then I'm manually doing some flag setup to get
the logging going the way I want it, with log files in a directory alongside the database directory. I'll probably at
some point want to write a similar logging facility for the SuperCollider side, because one thing I've seen for sure is
that it'd be nice to pull logs when there's been some server weirdness on that side and I can't for the life of me
imagine how the behavior I'm hearing described could happen with the code as written.

Boost is so mature now that I can just ```emerge``` it. But all that said I think I'll probably carry on with
```gflags```, ```glog```, and ```googletest``` for the moment. Enough messing with infrastructure for now. I've just
gotten the database kicking, and haven't even started to look at the three other big pieces of the project, the RESTful
web server with Pistache (maybe even with SSL although that's completely undocumented), the OSC endpoint, and the
supporting SuperCollider code. There's also that project to populate the database with the Twitter emoji images, as
well as building a composite of them at lower resolution to let folks pick them, still to come.

But for now, as always, it's time for bed.

