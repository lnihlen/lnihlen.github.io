---
layout: post
title: "Mocking"
date: 2019-05-02
tags: [ personal, sclork ]
---

Uneventful (if overfull) day at work, but I'm mostly thinking about my asset project for
{% include tag_link.html tag="sclork" %}.

Feeling enough confidence in the overall design direction of the ```Confab::Database``` object, at least for startup
verification and reading of assets, that I wanted to start adding some tests this evening. So I took a bit of a detour
to figure out how to get ```googlemock``` working, thinking I could mock a few of the objects in the leveldb library,
inject them into the ```Database```, and then start checking some expectations in the Database code.

The more I looked at it though the less confident I became that I'm going the right direction with either this database
object or the testing strategy. The two usually are pretty well linked, in that I have found over the years that the
easier an object is to test usually the better designed overall it is. A lot of the tests I can envision for the current
Database are mostly just change-detection tests, because much of what the Database is doing, at least on object
retrieval, is acting as some pretty thin veneer between it, flatbuffers, and the calling code.

So the conclusion is lookign at a complex mocking library was premature. In fact, this is my finding almost *every* time
I start going down the mocking path, I don't want a mock, I usually either want a stub or a fake. Now those words are
all pretty fuzzy, but in my usage a mock is a version of a dependency of the object under test that has some logic in
it allowing the tester to set *expectations* about which methods will be called, in which order, and with which
arguments. A fake for me is a lighter-weight thing that just overrides any methods in the dependent object and lets
the tester do whatever behavior they want, without any expectations expressed in a systematic way. And a stub is best
thought of as a fake where all of the function calls are just stubs, meaning no-ops. They do nothing save perhaps return
some default values.

But even that is sort of just change-detection testing. What I mean is that I feel that writing tests that verify
specific program behavior, as opposed to requirements, is often a huge waste of time. Implementation details need to
be correctly verified some times, but only to the extent that they serve the overall goal of program behavior.

For example, I could write a test of the Database object, using ```gmock```, that would verify that the initialization
routine of a new database writes a key with some data, and I could verify the specific bytes in both the key and the
data. Or, I could write the same test to check that the intialization routine writes something, and will return an error
if the database returns a read error or returns data that has already been written under that same key.

Probably a lot of similar mechanics involved in testing either case, and in fact the code might look quite similar for
both of those cases, but they have a difference in emphasis that is important. I do find myself often tempted to slip
back into verifying program mechanics, but the more I can focus on the *what* over the *how*, the better the tests are
and, by extension, the better the design of the API.

In the case of LevelDB all of the interfaces on the important objects are abstract, giving me my choice of
mock/fake/stub implementations, which is pretty cool. I think I'll start with a list of test cases in the code, then
decide which approach to take with the LevelDB dependency. I had actually some vague recollection of reading about a
way to get LevelDB to create entirely in-memory databases, but apparently that was a mistake. It looks like it could
be possibly doable by abusing the ```leveldb::Env``` object but that's much more of an elaborate test setup than is
really needed here. I am reminded that the goal of the testing is not to test LevelDB, but rather the bits of the
wrapper I've written around it,

But now it's late again, so time to try and make up some sleep.

